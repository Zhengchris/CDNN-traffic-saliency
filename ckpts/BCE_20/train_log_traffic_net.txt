2017-10-08 17:06:42,501 -------------- New training session, LR = 0.001000 ----------------
2017-10-08 17:06:42,501 -- length of training images = 49090--length of valid images = 6660--
2017-10-08 17:06:42,501 -- length of test images = 19140--
2017-10-08 17:08:45,553 -------------- New training session, LR = 0.001000 ----------------
2017-10-08 17:08:45,553 -- length of training images = 49090--length of valid images = 6660--
2017-10-08 17:08:45,554 -- length of test images = 19140--
2017-10-08 17:15:25,579 Training Iter 1000 Loss 0.056808 in 400.025s
2017-10-08 17:21:58,958 Training Iter 2000 Loss 0.043767 in 393.380s
2017-10-08 17:28:32,199 Training Iter 3000 Loss 0.039247 in 393.241s
2017-10-08 17:35:05,054 Training Iter 4000 Loss 0.036884 in 392.855s
2017-10-08 17:38:53,737 Epoch: 01 Train loss 0.0367 | Valid loss 0.0335
2017-10-08 17:45:29,902 Training Iter 1000 Loss 0.029706 in 396.165s
2017-10-08 17:52:02,660 Training Iter 2000 Loss 0.029636 in 392.758s
2017-10-08 17:58:35,353 Training Iter 3000 Loss 0.029586 in 392.692s
2017-10-08 18:05:07,977 Training Iter 4000 Loss 0.029554 in 392.624s
2017-10-08 18:08:50,691 Epoch: 02 Train loss 0.0295 | Valid loss 0.0336
2017-10-08 18:15:26,973 Training Iter 1000 Loss 0.029418 in 396.282s
2017-10-08 18:21:58,442 Training Iter 2000 Loss 0.029282 in 391.468s
2017-10-08 18:28:30,026 Training Iter 3000 Loss 0.029291 in 391.584s
2017-10-08 18:35:03,082 Training Iter 4000 Loss 0.029217 in 393.056s
2017-10-08 18:38:43,902 Epoch: 03 Train loss 0.0292 | Valid loss 0.0337
2017-10-08 18:45:20,574 Training Iter 1000 Loss 0.029096 in 396.671s
2017-10-08 18:51:53,729 Training Iter 2000 Loss 0.029148 in 393.155s
2017-10-08 18:58:26,742 Training Iter 3000 Loss 0.029093 in 393.013s
2017-10-08 19:04:59,732 Training Iter 4000 Loss 0.029083 in 392.989s
2017-10-08 19:08:40,451 Epoch: 04 Train loss 0.0291 | Valid loss 0.0348
2017-10-08 19:15:16,950 Training Iter 1000 Loss 0.028949 in 396.499s
2017-10-08 19:21:50,021 Training Iter 2000 Loss 0.029001 in 393.070s
2017-10-08 19:28:23,044 Training Iter 3000 Loss 0.029045 in 393.023s
2017-10-08 19:34:55,941 Training Iter 4000 Loss 0.029046 in 392.897s
2017-10-08 19:38:34,716 Epoch: 05 Train loss 0.0291 | Valid loss 0.0334
2017-10-08 19:45:09,660 Training Iter 1000 Loss 0.028785 in 394.943s
2017-10-08 19:51:41,204 Training Iter 2000 Loss 0.028967 in 391.544s
2017-10-08 19:58:12,862 Training Iter 3000 Loss 0.029020 in 391.658s
2017-10-08 20:04:44,404 Training Iter 4000 Loss 0.029026 in 391.543s
2017-10-08 20:08:22,871 Epoch: 06 Train loss 0.0290 | Valid loss 0.0328
2017-10-08 20:14:57,702 Training Iter 1000 Loss 0.028952 in 394.831s
2017-10-08 20:21:29,194 Training Iter 2000 Loss 0.028921 in 391.492s
2017-10-08 20:28:00,692 Training Iter 3000 Loss 0.028946 in 391.497s
2017-10-08 20:34:32,244 Training Iter 4000 Loss 0.028993 in 391.552s
2017-10-08 20:38:11,174 Epoch: 07 Train loss 0.0290 | Valid loss 0.0324
2017-10-08 20:44:45,941 Training Iter 1000 Loss 0.029023 in 394.766s
2017-10-08 20:51:17,369 Training Iter 2000 Loss 0.028960 in 391.428s
2017-10-08 20:57:48,828 Training Iter 3000 Loss 0.028987 in 391.459s
2017-10-08 21:04:20,242 Training Iter 4000 Loss 0.028969 in 391.414s
2017-10-08 21:07:58,655 Epoch: 08 Train loss 0.0290 | Valid loss 0.0357
2017-10-08 21:14:33,651 Training Iter 1000 Loss 0.029030 in 394.995s
2017-10-08 21:21:05,112 Training Iter 2000 Loss 0.029047 in 391.461s
2017-10-08 21:27:36,617 Training Iter 3000 Loss 0.029053 in 391.505s
2017-10-08 21:34:08,121 Training Iter 4000 Loss 0.028963 in 391.504s
2017-10-08 21:37:46,984 Epoch: 09 Train loss 0.0290 | Valid loss 0.0325
2017-10-08 21:44:21,961 Training Iter 1000 Loss 0.028893 in 394.977s
2017-10-08 21:50:53,470 Training Iter 2000 Loss 0.029014 in 391.508s
2017-10-08 21:57:25,012 Training Iter 3000 Loss 0.028989 in 391.542s
2017-10-08 22:03:56,579 Training Iter 4000 Loss 0.028983 in 391.567s
2017-10-08 22:07:35,075 Epoch: 10 Train loss 0.0290 | Valid loss 0.0328
2017-10-08 22:14:09,981 Training Iter 1000 Loss 0.028973 in 394.905s
2017-10-08 22:20:41,536 Training Iter 2000 Loss 0.028957 in 391.555s
2017-10-08 22:27:13,025 Training Iter 3000 Loss 0.028931 in 391.488s
2017-10-08 22:33:44,428 Training Iter 4000 Loss 0.028941 in 391.403s
2017-10-08 22:37:22,724 Epoch: 11 Train loss 0.0289 | Valid loss 0.0336
2017-10-08 22:43:57,702 Training Iter 1000 Loss 0.029005 in 394.978s
2017-10-08 22:50:29,018 Training Iter 2000 Loss 0.028931 in 391.316s
2017-10-08 22:57:00,448 Training Iter 3000 Loss 0.028894 in 391.430s
2017-10-08 23:03:31,855 Training Iter 4000 Loss 0.028920 in 391.407s
2017-10-08 23:07:10,390 Epoch: 12 Train loss 0.0289 | Valid loss 0.0329
2017-10-08 23:13:45,811 Training Iter 1000 Loss 0.029052 in 395.420s
2017-10-08 23:20:18,337 Training Iter 2000 Loss 0.028997 in 392.527s
2017-10-08 23:26:50,758 Training Iter 3000 Loss 0.028959 in 392.421s
2017-10-08 23:33:23,110 Training Iter 4000 Loss 0.028928 in 392.351s
2017-10-08 23:37:04,361 Epoch: 13 Train loss 0.0289 | Valid loss 0.0331
2017-10-08 23:43:40,214 Training Iter 1000 Loss 0.028882 in 395.852s
2017-10-08 23:50:12,624 Training Iter 2000 Loss 0.028930 in 392.410s
2017-10-08 23:56:44,905 Training Iter 3000 Loss 0.028885 in 392.281s
2017-10-09 00:03:17,179 Training Iter 4000 Loss 0.028907 in 392.273s
2017-10-09 00:06:58,297 Epoch: 14 Train loss 0.0289 | Valid loss 0.0346
2017-10-09 00:13:34,833 Training Iter 1000 Loss 0.028865 in 396.535s
2017-10-09 00:20:06,214 Training Iter 2000 Loss 0.028924 in 391.381s
2017-10-09 00:26:37,670 Training Iter 3000 Loss 0.028907 in 391.456s
2017-10-09 00:33:09,085 Training Iter 4000 Loss 0.028922 in 391.414s
2017-10-09 00:36:47,613 Epoch: 15 Train loss 0.0289 | Valid loss 0.0360
2017-10-09 00:43:22,441 Training Iter 1000 Loss 0.028923 in 394.827s
2017-10-09 00:49:53,862 Training Iter 2000 Loss 0.028891 in 391.421s
2017-10-09 00:56:25,259 Training Iter 3000 Loss 0.028924 in 391.397s
2017-10-09 01:02:56,662 Training Iter 4000 Loss 0.028919 in 391.403s
2017-10-09 01:06:35,370 Epoch: 16 Train loss 0.0289 | Valid loss 0.0332
2017-10-09 01:13:10,125 Training Iter 1000 Loss 0.028890 in 394.754s
2017-10-09 01:19:41,417 Training Iter 2000 Loss 0.028966 in 391.292s
2017-10-09 01:26:12,608 Training Iter 3000 Loss 0.028954 in 391.191s
2017-10-09 01:32:43,790 Training Iter 4000 Loss 0.028896 in 391.182s
2017-10-09 01:36:22,710 Epoch: 17 Train loss 0.0289 | Valid loss 0.0328
2017-10-09 01:42:57,271 Training Iter 1000 Loss 0.028761 in 394.561s
2017-10-09 01:49:28,674 Training Iter 2000 Loss 0.028833 in 391.403s
2017-10-09 01:56:00,028 Training Iter 3000 Loss 0.028895 in 391.354s
2017-10-09 02:02:31,393 Training Iter 4000 Loss 0.028896 in 391.365s
2017-10-09 02:06:10,270 Epoch: 18 Train loss 0.0289 | Valid loss 0.0336
2017-10-09 02:12:44,946 Training Iter 1000 Loss 0.028958 in 394.676s
2017-10-09 02:19:16,508 Training Iter 2000 Loss 0.028934 in 391.562s
2017-10-09 02:25:47,780 Training Iter 3000 Loss 0.028881 in 391.272s
2017-10-09 02:32:19,095 Training Iter 4000 Loss 0.028886 in 391.315s
2017-10-09 02:35:57,876 Epoch: 19 Train loss 0.0289 | Valid loss 0.0344
2017-10-09 02:42:32,574 Training Iter 1000 Loss 0.028836 in 394.698s
2017-10-09 02:49:03,953 Training Iter 2000 Loss 0.028829 in 391.379s
2017-10-09 02:55:35,259 Training Iter 3000 Loss 0.028863 in 391.306s
2017-10-09 03:02:06,623 Training Iter 4000 Loss 0.028861 in 391.364s
2017-10-09 03:05:44,888 Epoch: 20 Train loss 0.0289 | Valid loss 0.0342
2017-10-09 03:12:19,599 Training Iter 1000 Loss 0.028914 in 394.710s
2017-10-09 03:18:50,838 Training Iter 2000 Loss 0.028887 in 391.239s
2017-10-09 03:25:22,108 Training Iter 3000 Loss 0.028894 in 391.270s
2017-10-09 03:31:53,407 Training Iter 4000 Loss 0.028890 in 391.299s
2017-10-09 03:35:31,909 Epoch: 21 Train loss 0.0289 | Valid loss 0.0341
2017-10-09 03:42:06,587 Training Iter 1000 Loss 0.028781 in 394.677s
2017-10-09 03:48:37,830 Training Iter 2000 Loss 0.028813 in 391.243s
2017-10-09 03:55:09,064 Training Iter 3000 Loss 0.028878 in 391.234s
2017-10-09 04:01:40,367 Training Iter 4000 Loss 0.028879 in 391.302s
2017-10-09 04:05:18,509 Epoch: 22 Train loss 0.0289 | Valid loss 0.0331
2017-10-09 04:11:53,364 Training Iter 1000 Loss 0.028851 in 394.855s
2017-10-09 04:18:24,715 Training Iter 2000 Loss 0.028865 in 391.351s
2017-10-09 04:24:55,990 Training Iter 3000 Loss 0.028879 in 391.275s
2017-10-09 04:31:27,304 Training Iter 4000 Loss 0.028869 in 391.314s
2017-10-09 04:35:05,801 Epoch: 23 Train loss 0.0289 | Valid loss 0.0326
2017-10-09 04:41:40,880 Training Iter 1000 Loss 0.028946 in 395.078s
2017-10-09 04:48:12,201 Training Iter 2000 Loss 0.028861 in 391.321s
2017-10-09 04:54:43,553 Training Iter 3000 Loss 0.028837 in 391.353s
2017-10-09 05:01:14,760 Training Iter 4000 Loss 0.028883 in 391.207s
2017-10-09 05:04:53,113 Epoch: 24 Train loss 0.0289 | Valid loss 0.0344
2017-10-09 05:11:27,621 Training Iter 1000 Loss 0.028810 in 394.507s
2017-10-09 05:17:58,922 Training Iter 2000 Loss 0.028836 in 391.302s
2017-10-09 05:24:30,103 Training Iter 3000 Loss 0.028827 in 391.181s
2017-10-09 05:31:01,375 Training Iter 4000 Loss 0.028868 in 391.273s
2017-10-09 05:34:39,538 Epoch: 25 Train loss 0.0289 | Valid loss 0.0342
2017-10-09 05:41:14,218 Training Iter 1000 Loss 0.028797 in 394.680s
2017-10-09 05:47:45,670 Training Iter 2000 Loss 0.028915 in 391.452s
2017-10-09 05:54:17,003 Training Iter 3000 Loss 0.028856 in 391.333s
2017-10-09 06:00:48,380 Training Iter 4000 Loss 0.028874 in 391.378s
2017-10-09 06:04:27,088 Epoch: 26 Train loss 0.0289 | Valid loss 0.0342
2017-10-09 06:11:02,061 Training Iter 1000 Loss 0.028869 in 394.972s
2017-10-09 06:17:33,382 Training Iter 2000 Loss 0.028853 in 391.321s
2017-10-09 06:24:04,654 Training Iter 3000 Loss 0.028851 in 391.272s
2017-10-09 06:30:35,819 Training Iter 4000 Loss 0.028876 in 391.165s
2017-10-09 06:34:14,004 Epoch: 27 Train loss 0.0289 | Valid loss 0.0342
2017-10-09 06:40:48,612 Training Iter 1000 Loss 0.028719 in 394.608s
2017-10-09 06:47:19,819 Training Iter 2000 Loss 0.028675 in 391.207s
2017-10-09 06:53:51,104 Training Iter 3000 Loss 0.028823 in 391.285s
2017-10-09 07:00:22,479 Training Iter 4000 Loss 0.028869 in 391.375s
2017-10-09 07:04:00,651 Epoch: 28 Train loss 0.0289 | Valid loss 0.0327
2017-10-09 07:10:35,292 Training Iter 1000 Loss 0.028814 in 394.640s
2017-10-09 07:17:06,731 Training Iter 2000 Loss 0.028824 in 391.439s
2017-10-09 07:23:38,162 Training Iter 3000 Loss 0.028879 in 391.431s
2017-10-09 07:30:09,559 Training Iter 4000 Loss 0.028891 in 391.397s
2017-10-09 07:33:48,149 Epoch: 29 Train loss 0.0289 | Valid loss 0.0326
2017-10-09 07:40:22,915 Training Iter 1000 Loss 0.028816 in 394.765s
2017-10-09 07:46:54,244 Training Iter 2000 Loss 0.028760 in 391.329s
2017-10-09 07:53:25,593 Training Iter 3000 Loss 0.028818 in 391.349s
2017-10-09 07:59:56,832 Training Iter 4000 Loss 0.028871 in 391.239s
2017-10-09 08:03:34,386 Epoch: 30 Train loss 0.0289 | Valid loss 0.0350
2017-10-09 08:10:08,924 Training Iter 1000 Loss 0.028823 in 394.538s
2017-10-09 08:16:40,098 Training Iter 2000 Loss 0.028761 in 391.173s
2017-10-09 08:23:11,329 Training Iter 3000 Loss 0.028781 in 391.232s
2017-10-09 08:29:42,606 Training Iter 4000 Loss 0.028864 in 391.276s
2017-10-09 08:33:20,388 Epoch: 31 Train loss 0.0289 | Valid loss 0.0326
2017-10-09 08:39:55,040 Training Iter 1000 Loss 0.028942 in 394.652s
2017-10-09 08:46:26,390 Training Iter 2000 Loss 0.028964 in 391.350s
2017-10-09 08:52:57,786 Training Iter 3000 Loss 0.028965 in 391.395s
2017-10-09 08:59:29,078 Training Iter 4000 Loss 0.028871 in 391.292s
2017-10-09 09:03:07,597 Epoch: 32 Train loss 0.0289 | Valid loss 0.0342
2017-10-09 09:09:42,286 Training Iter 1000 Loss 0.028722 in 394.689s
2017-10-09 09:16:13,409 Training Iter 2000 Loss 0.028871 in 391.123s
2017-10-09 09:22:44,533 Training Iter 3000 Loss 0.028845 in 391.123s
2017-10-09 09:29:15,687 Training Iter 4000 Loss 0.028866 in 391.154s
2017-10-09 09:32:53,994 Epoch: 33 Train loss 0.0289 | Valid loss 0.0332
2017-10-09 09:39:28,683 Training Iter 1000 Loss 0.028971 in 394.688s
2017-10-09 09:45:59,879 Training Iter 2000 Loss 0.028973 in 391.196s
2017-10-09 09:52:31,090 Training Iter 3000 Loss 0.028918 in 391.212s
2017-10-09 09:59:02,687 Training Iter 4000 Loss 0.028865 in 391.597s
2017-10-09 10:02:43,157 Epoch: 34 Train loss 0.0289 | Valid loss 0.0374
2017-10-09 10:09:19,238 Training Iter 1000 Loss 0.028871 in 396.080s
2017-10-09 10:15:51,887 Training Iter 2000 Loss 0.028849 in 392.649s
2017-10-09 10:22:24,326 Training Iter 3000 Loss 0.028881 in 392.439s
2017-10-09 10:28:57,026 Training Iter 4000 Loss 0.028883 in 392.700s
2017-10-09 10:32:38,197 Epoch: 35 Train loss 0.0289 | Valid loss 0.0328
2017-10-09 10:39:12,531 Training Iter 1000 Loss 0.028979 in 394.334s
2017-10-09 10:45:43,128 Training Iter 2000 Loss 0.029004 in 390.596s
2017-10-09 10:52:14,230 Training Iter 3000 Loss 0.028859 in 391.102s
2017-10-09 10:58:44,925 Training Iter 4000 Loss 0.028862 in 390.695s
2017-10-09 11:02:24,914 Epoch: 36 Train loss 0.0289 | Valid loss 0.0339
2017-10-09 11:08:58,745 Training Iter 1000 Loss 0.028900 in 393.831s
2017-10-09 11:15:28,724 Training Iter 2000 Loss 0.028843 in 389.978s
2017-10-09 11:21:57,600 Training Iter 3000 Loss 0.028863 in 388.876s
2017-10-09 11:28:26,915 Training Iter 4000 Loss 0.028858 in 389.315s
2017-10-09 11:32:06,637 Epoch: 37 Train loss 0.0289 | Valid loss 0.0345
2017-10-09 11:38:40,117 Training Iter 1000 Loss 0.028840 in 393.479s
2017-10-09 11:45:10,126 Training Iter 2000 Loss 0.028791 in 390.009s
2017-10-09 11:51:39,982 Training Iter 3000 Loss 0.028816 in 389.856s
2017-10-09 11:58:09,807 Training Iter 4000 Loss 0.028834 in 389.825s
2017-10-09 12:01:49,245 Epoch: 38 Train loss 0.0289 | Valid loss 0.0332
2017-10-09 12:08:22,545 Training Iter 1000 Loss 0.028793 in 393.299s
2017-10-09 12:14:52,357 Training Iter 2000 Loss 0.028860 in 389.813s
2017-10-09 12:21:22,200 Training Iter 3000 Loss 0.028850 in 389.843s
2017-10-09 12:27:52,045 Training Iter 4000 Loss 0.028854 in 389.845s
2017-10-09 12:31:32,228 Epoch: 39 Train loss 0.0289 | Valid loss 0.0333
2017-10-09 12:38:05,706 Training Iter 1000 Loss 0.028832 in 393.477s
2017-10-09 12:44:35,699 Training Iter 2000 Loss 0.028856 in 389.993s
2017-10-09 12:51:05,566 Training Iter 3000 Loss 0.028843 in 389.867s
2017-10-09 12:57:35,501 Training Iter 4000 Loss 0.028851 in 389.935s
2017-10-09 13:01:15,405 Epoch: 40 Train loss 0.0289 | Valid loss 0.0334
2017-10-09 13:07:48,827 Training Iter 1000 Loss 0.028883 in 393.422s
2017-10-09 13:14:18,950 Training Iter 2000 Loss 0.028867 in 390.123s
2017-10-09 13:20:49,033 Training Iter 3000 Loss 0.028883 in 390.082s
2017-10-09 13:27:19,074 Training Iter 4000 Loss 0.028864 in 390.041s
2017-10-09 13:30:56,382 Epoch: 41 Train loss 0.0289 | Valid loss 0.0338
2017-10-09 13:37:30,158 Training Iter 1000 Loss 0.028979 in 393.776s
2017-10-09 13:44:00,234 Training Iter 2000 Loss 0.028868 in 390.075s
2017-10-09 13:50:30,281 Training Iter 3000 Loss 0.028882 in 390.047s
2017-10-09 13:57:00,403 Training Iter 4000 Loss 0.028857 in 390.122s
2017-10-09 14:00:37,572 Epoch: 42 Train loss 0.0289 | Valid loss 0.0345
2017-10-09 14:07:10,938 Training Iter 1000 Loss 0.028904 in 393.366s
2017-10-09 14:13:41,030 Training Iter 2000 Loss 0.028917 in 390.092s
2017-10-09 14:20:11,167 Training Iter 3000 Loss 0.028858 in 390.137s
2017-10-09 14:26:41,323 Training Iter 4000 Loss 0.028857 in 390.156s
2017-10-09 14:30:19,558 Epoch: 43 Train loss 0.0289 | Valid loss 0.0327
2017-10-09 14:36:52,882 Training Iter 1000 Loss 0.028860 in 393.323s
2017-10-09 14:43:22,938 Training Iter 2000 Loss 0.028888 in 390.056s
2017-10-09 14:49:53,085 Training Iter 3000 Loss 0.028858 in 390.147s
2017-10-09 14:56:23,150 Training Iter 4000 Loss 0.028858 in 390.066s
2017-10-09 15:00:00,744 Epoch: 44 Train loss 0.0289 | Valid loss 0.0331
2017-10-09 15:06:34,142 Training Iter 1000 Loss 0.029007 in 393.398s
2017-10-09 15:13:05,419 Training Iter 2000 Loss 0.028891 in 391.277s
2017-10-09 15:20:54,709 Training Iter 3000 Loss 0.028823 in 469.290s
2017-10-09 15:27:43,561 Training Iter 4000 Loss 0.028861 in 408.852s
2017-10-09 15:31:27,084 Epoch: 45 Train loss 0.0289 | Valid loss 0.0351
2017-10-09 15:38:00,350 Training Iter 1000 Loss 0.028878 in 393.266s
2017-10-09 15:44:30,206 Training Iter 2000 Loss 0.028940 in 389.856s
2017-10-09 15:51:00,190 Training Iter 3000 Loss 0.028862 in 389.983s
2017-10-09 15:57:30,110 Training Iter 4000 Loss 0.028861 in 389.920s
2017-10-09 16:01:09,696 Epoch: 46 Train loss 0.0289 | Valid loss 0.0338
2017-10-09 16:07:44,872 Training Iter 1000 Loss 0.028980 in 395.176s
2017-10-09 16:14:14,816 Training Iter 2000 Loss 0.028880 in 389.944s
2017-10-09 16:20:44,588 Training Iter 3000 Loss 0.028860 in 389.772s
2017-10-09 16:27:14,387 Training Iter 4000 Loss 0.028863 in 389.799s
2017-10-09 16:30:54,179 Epoch: 47 Train loss 0.0288 | Valid loss 0.0339
2017-10-09 16:37:32,164 Training Iter 1000 Loss 0.029004 in 397.984s
2017-10-09 16:44:02,317 Training Iter 2000 Loss 0.028932 in 390.153s
2017-10-09 16:50:37,352 Training Iter 3000 Loss 0.028865 in 395.035s
2017-10-09 16:57:12,968 Training Iter 4000 Loss 0.028860 in 395.616s
2017-10-09 17:00:55,838 Epoch: 48 Train loss 0.0289 | Valid loss 0.0340
2017-10-09 17:07:31,812 Training Iter 1000 Loss 0.028892 in 395.974s
2017-10-09 17:14:05,665 Training Iter 2000 Loss 0.028848 in 393.853s
2017-10-09 17:20:37,692 Training Iter 3000 Loss 0.028849 in 392.027s
2017-10-09 17:27:07,907 Training Iter 4000 Loss 0.028864 in 390.214s
2017-10-09 17:30:48,158 Epoch: 49 Train loss 0.0289 | Valid loss 0.0326
2017-10-09 17:37:21,625 Training Iter 1000 Loss 0.028784 in 393.467s
2017-10-09 17:43:51,685 Training Iter 2000 Loss 0.028770 in 390.060s
2017-10-09 17:50:21,658 Training Iter 3000 Loss 0.028787 in 389.973s
2017-10-09 17:56:51,610 Training Iter 4000 Loss 0.028852 in 389.952s
2017-10-09 18:00:31,895 Epoch: 50 Train loss 0.0289 | Valid loss 0.0325
2017-10-09 18:07:05,116 Training Iter 1000 Loss 0.028692 in 393.221s
2017-10-09 18:13:34,874 Training Iter 2000 Loss 0.028807 in 389.757s
2017-10-09 18:20:04,278 Training Iter 3000 Loss 0.028883 in 389.404s
2017-10-09 18:26:33,714 Training Iter 4000 Loss 0.028861 in 389.436s
2017-10-09 18:30:10,925 Epoch: 51 Train loss 0.0289 | Valid loss 0.0350
2017-10-09 18:36:43,735 Training Iter 1000 Loss 0.028883 in 392.810s
2017-10-09 18:43:13,295 Training Iter 2000 Loss 0.028864 in 389.560s
2017-10-09 18:49:42,816 Training Iter 3000 Loss 0.028797 in 389.521s
2017-10-09 18:56:12,365 Training Iter 4000 Loss 0.028855 in 389.550s
2017-10-09 18:59:49,031 Epoch: 52 Train loss 0.0289 | Valid loss 0.0356
2017-10-09 19:06:22,056 Training Iter 1000 Loss 0.028931 in 393.025s
2017-10-09 19:12:51,201 Training Iter 2000 Loss 0.028890 in 389.145s
2017-10-09 19:19:18,966 Training Iter 3000 Loss 0.028911 in 387.765s
2017-10-09 19:25:46,737 Training Iter 4000 Loss 0.028869 in 387.772s
2017-10-09 19:29:23,551 Epoch: 53 Train loss 0.0289 | Valid loss 0.0327
2017-10-09 19:35:54,770 Training Iter 1000 Loss 0.028829 in 391.218s
2017-10-09 19:42:22,466 Training Iter 2000 Loss 0.028868 in 387.697s
2017-10-09 19:48:50,172 Training Iter 3000 Loss 0.028819 in 387.706s
2017-10-09 19:55:17,925 Training Iter 4000 Loss 0.028862 in 387.753s
2017-10-09 19:58:53,401 Epoch: 54 Train loss 0.0289 | Valid loss 0.0329
2017-10-09 20:05:24,738 Training Iter 1000 Loss 0.028854 in 391.336s
2017-10-09 20:11:52,487 Training Iter 2000 Loss 0.028870 in 387.750s
2017-10-09 20:18:20,217 Training Iter 3000 Loss 0.028850 in 387.730s
2017-10-09 20:24:48,008 Training Iter 4000 Loss 0.028867 in 387.790s
2017-10-09 20:28:24,006 Epoch: 55 Train loss 0.0289 | Valid loss 0.0323
2017-10-09 20:34:55,246 Training Iter 1000 Loss 0.028949 in 391.240s
2017-10-09 20:41:23,167 Training Iter 2000 Loss 0.028854 in 387.920s
2017-10-09 20:47:51,033 Training Iter 3000 Loss 0.028891 in 387.867s
2017-10-09 20:54:18,786 Training Iter 4000 Loss 0.028853 in 387.753s
2017-10-09 20:57:54,577 Epoch: 56 Train loss 0.0289 | Valid loss 0.0332
2017-10-09 21:04:25,863 Training Iter 1000 Loss 0.028849 in 391.285s
2017-10-09 21:10:53,616 Training Iter 2000 Loss 0.028841 in 387.753s
2017-10-09 21:17:21,450 Training Iter 3000 Loss 0.028868 in 387.833s
2017-10-09 21:23:49,367 Training Iter 4000 Loss 0.028867 in 387.918s
2017-10-09 21:27:24,979 Epoch: 57 Train loss 0.0289 | Valid loss 0.0339
2017-10-09 21:33:56,469 Training Iter 1000 Loss 0.029018 in 391.489s
2017-10-09 21:40:24,279 Training Iter 2000 Loss 0.028955 in 387.810s
2017-10-09 21:46:52,169 Training Iter 3000 Loss 0.028815 in 387.890s
2017-10-09 21:53:19,979 Training Iter 4000 Loss 0.028854 in 387.810s
2017-10-09 21:56:55,173 Epoch: 58 Train loss 0.0289 | Valid loss 0.0328
2017-10-09 22:03:26,370 Training Iter 1000 Loss 0.028987 in 391.196s
2017-10-09 22:09:54,138 Training Iter 2000 Loss 0.028920 in 387.768s
2017-10-09 22:16:21,896 Training Iter 3000 Loss 0.028870 in 387.758s
2017-10-09 22:22:49,642 Training Iter 4000 Loss 0.028880 in 387.746s
2017-10-09 22:26:24,045 Epoch: 59 Train loss 0.0289 | Valid loss 0.0333
2017-10-09 22:32:55,456 Training Iter 1000 Loss 0.028684 in 391.410s
2017-10-09 22:39:23,280 Training Iter 2000 Loss 0.028805 in 387.824s
2017-10-09 22:45:51,011 Training Iter 3000 Loss 0.028843 in 387.732s
2017-10-09 22:52:18,715 Training Iter 4000 Loss 0.028875 in 387.704s
2017-10-09 22:55:54,241 Epoch: 60 Train loss 0.0289 | Valid loss 0.0330
2017-10-09 23:02:25,644 Training Iter 1000 Loss 0.028864 in 391.403s
2017-10-09 23:08:53,439 Training Iter 2000 Loss 0.028940 in 387.796s
2017-10-09 23:15:21,204 Training Iter 3000 Loss 0.028899 in 387.765s
2017-10-09 23:21:48,962 Training Iter 4000 Loss 0.028867 in 387.758s
2017-10-09 23:25:23,900 Epoch: 61 Train loss 0.0289 | Valid loss 0.0338
2017-10-09 23:31:55,188 Training Iter 1000 Loss 0.028838 in 391.287s
2017-10-09 23:38:23,007 Training Iter 2000 Loss 0.028847 in 387.820s
2017-10-09 23:44:50,913 Training Iter 3000 Loss 0.028850 in 387.906s
2017-10-09 23:51:18,763 Training Iter 4000 Loss 0.028868 in 387.850s
2017-10-09 23:54:53,358 Epoch: 62 Train loss 0.0289 | Valid loss 0.0327
2017-10-10 00:01:24,737 Training Iter 1000 Loss 0.029067 in 391.378s
2017-10-10 00:07:52,425 Training Iter 2000 Loss 0.028954 in 387.689s
2017-10-10 00:14:20,084 Training Iter 3000 Loss 0.028900 in 387.659s
2017-10-10 00:20:47,773 Training Iter 4000 Loss 0.028875 in 387.690s
2017-10-10 00:24:21,698 Epoch: 63 Train loss 0.0289 | Valid loss 0.0348
2017-10-10 00:30:52,697 Training Iter 1000 Loss 0.028767 in 390.998s
2017-10-10 00:37:20,360 Training Iter 2000 Loss 0.028892 in 387.663s
2017-10-10 00:43:48,018 Training Iter 3000 Loss 0.028895 in 387.658s
2017-10-10 00:50:15,675 Training Iter 4000 Loss 0.028862 in 387.657s
2017-10-10 00:53:49,831 Epoch: 64 Train loss 0.0289 | Valid loss 0.0335
2017-10-10 01:00:20,776 Training Iter 1000 Loss 0.028846 in 390.944s
2017-10-10 01:06:48,450 Training Iter 2000 Loss 0.028834 in 387.674s
2017-10-10 01:13:16,123 Training Iter 3000 Loss 0.028840 in 387.674s
2017-10-10 01:19:43,759 Training Iter 4000 Loss 0.028854 in 387.635s
2017-10-10 01:23:17,515 Epoch: 65 Train loss 0.0289 | Valid loss 0.0331
2017-10-10 01:29:48,757 Training Iter 1000 Loss 0.028927 in 391.241s
2017-10-10 01:36:16,328 Training Iter 2000 Loss 0.028871 in 387.571s
2017-10-10 01:42:43,929 Training Iter 3000 Loss 0.028879 in 387.601s
2017-10-10 01:49:11,532 Training Iter 4000 Loss 0.028872 in 387.603s
2017-10-10 01:52:45,144 Epoch: 66 Train loss 0.0289 | Valid loss 0.0325
2017-10-10 01:59:16,197 Training Iter 1000 Loss 0.029047 in 391.053s
2017-10-10 02:05:43,795 Training Iter 2000 Loss 0.028972 in 387.597s
2017-10-10 02:12:11,361 Training Iter 3000 Loss 0.028902 in 387.566s
2017-10-10 02:18:38,944 Training Iter 4000 Loss 0.028845 in 387.583s
2017-10-10 02:22:12,725 Epoch: 67 Train loss 0.0289 | Valid loss 0.0333
2017-10-10 02:28:43,598 Training Iter 1000 Loss 0.028779 in 390.862s
2017-10-10 02:35:11,296 Training Iter 2000 Loss 0.028884 in 387.698s
2017-10-10 02:41:38,964 Training Iter 3000 Loss 0.028867 in 387.668s
2017-10-10 02:48:06,578 Training Iter 4000 Loss 0.028857 in 387.614s
2017-10-10 02:51:39,888 Epoch: 68 Train loss 0.0289 | Valid loss 0.0345
2017-10-10 02:58:10,863 Training Iter 1000 Loss 0.028932 in 390.974s
2017-10-10 03:04:38,410 Training Iter 2000 Loss 0.028875 in 387.547s
2017-10-10 03:11:05,935 Training Iter 3000 Loss 0.028835 in 387.525s
2017-10-10 03:17:33,559 Training Iter 4000 Loss 0.028858 in 387.624s
2017-10-10 03:21:06,235 Epoch: 69 Train loss 0.0289 | Valid loss 0.0325
2017-10-10 03:27:38,659 Training Iter 1000 Loss 0.028801 in 392.423s
2017-10-10 03:34:06,799 Training Iter 2000 Loss 0.028859 in 388.140s
2017-10-10 03:40:34,833 Training Iter 3000 Loss 0.028855 in 388.035s
2017-10-10 03:47:02,589 Training Iter 4000 Loss 0.028869 in 387.756s
2017-10-10 03:50:35,390 Epoch: 70 Train loss 0.0289 | Valid loss 0.0331
2017-10-10 03:57:06,491 Training Iter 1000 Loss 0.028746 in 391.101s
2017-10-10 04:03:34,545 Training Iter 2000 Loss 0.028836 in 388.054s
2017-10-10 04:10:02,279 Training Iter 3000 Loss 0.028855 in 387.734s
2017-10-10 04:16:29,963 Training Iter 4000 Loss 0.028853 in 387.684s
2017-10-10 04:20:01,698 Epoch: 71 Train loss 0.0289 | Valid loss 0.0333
2017-10-10 04:26:33,037 Training Iter 1000 Loss 0.029016 in 391.338s
2017-10-10 04:33:01,132 Training Iter 2000 Loss 0.028971 in 388.095s
2017-10-10 04:39:28,739 Training Iter 3000 Loss 0.028936 in 387.607s
2017-10-10 04:45:56,316 Training Iter 4000 Loss 0.028872 in 387.577s
2017-10-10 04:49:27,624 Epoch: 72 Train loss 0.0289 | Valid loss 0.0370
2017-10-10 04:55:58,638 Training Iter 1000 Loss 0.028927 in 391.014s
2017-10-10 05:02:26,185 Training Iter 2000 Loss 0.028850 in 387.547s
2017-10-10 05:08:54,184 Training Iter 3000 Loss 0.028871 in 387.999s
2017-10-10 05:15:21,728 Training Iter 4000 Loss 0.028857 in 387.543s
2017-10-10 05:18:52,863 Epoch: 73 Train loss 0.0289 | Valid loss 0.0333
2017-10-10 05:25:23,951 Training Iter 1000 Loss 0.028759 in 391.087s
2017-10-10 05:31:51,614 Training Iter 2000 Loss 0.028850 in 387.664s
2017-10-10 05:38:19,658 Training Iter 3000 Loss 0.028857 in 388.044s
2017-10-10 05:44:47,289 Training Iter 4000 Loss 0.028863 in 387.631s
2017-10-10 05:48:19,046 Epoch: 74 Train loss 0.0289 | Valid loss 0.0340
2017-10-10 05:54:50,304 Training Iter 1000 Loss 0.029066 in 391.257s
2017-10-10 06:01:17,886 Training Iter 2000 Loss 0.028913 in 387.582s
2017-10-10 06:07:45,844 Training Iter 3000 Loss 0.028886 in 387.958s
2017-10-10 06:14:13,450 Training Iter 4000 Loss 0.028872 in 387.606s
2017-10-10 06:17:43,856 Epoch: 75 Train loss 0.0289 | Valid loss 0.0333
2017-10-10 06:24:15,234 Training Iter 1000 Loss 0.028854 in 391.378s
2017-10-10 06:30:42,806 Training Iter 2000 Loss 0.028849 in 387.571s
2017-10-10 06:37:10,362 Training Iter 3000 Loss 0.028891 in 387.556s
2017-10-10 06:43:38,265 Training Iter 4000 Loss 0.028884 in 387.903s
2017-10-10 06:47:09,417 Epoch: 76 Train loss 0.0289 | Valid loss 0.0327
2017-10-10 06:53:40,287 Training Iter 1000 Loss 0.028859 in 390.869s
2017-10-10 07:00:08,200 Training Iter 2000 Loss 0.028793 in 387.913s
2017-10-10 07:06:35,822 Training Iter 3000 Loss 0.028859 in 387.622s
2017-10-10 07:13:03,777 Training Iter 4000 Loss 0.028869 in 387.955s
2017-10-10 07:16:34,233 Epoch: 77 Train loss 0.0289 | Valid loss 0.0376
2017-10-10 07:23:05,181 Training Iter 1000 Loss 0.028851 in 390.947s
2017-10-10 07:29:33,122 Training Iter 2000 Loss 0.028839 in 387.941s
2017-10-10 07:36:00,734 Training Iter 3000 Loss 0.028820 in 387.613s
2017-10-10 07:42:28,786 Training Iter 4000 Loss 0.028861 in 388.052s
2017-10-10 07:45:59,721 Epoch: 78 Train loss 0.0289 | Valid loss 0.0329
2017-10-10 07:52:30,639 Training Iter 1000 Loss 0.028926 in 390.918s
2017-10-10 07:58:58,600 Training Iter 2000 Loss 0.028857 in 387.961s
2017-10-10 08:05:26,199 Training Iter 3000 Loss 0.028840 in 387.598s
2017-10-10 08:11:53,970 Training Iter 4000 Loss 0.028857 in 387.771s
2017-10-10 08:15:24,117 Epoch: 79 Train loss 0.0289 | Valid loss 0.0341
2017-10-10 08:21:55,196 Training Iter 1000 Loss 0.028725 in 391.079s
2017-10-10 08:28:23,208 Training Iter 2000 Loss 0.028740 in 388.012s
2017-10-10 08:34:50,897 Training Iter 3000 Loss 0.028853 in 387.689s
2017-10-10 08:41:18,558 Training Iter 4000 Loss 0.028864 in 387.661s
2017-10-10 08:44:50,228 Epoch: 80 Train loss 0.0289 | Valid loss 0.0326
2017-10-10 08:51:21,229 Training Iter 1000 Loss 0.028916 in 391.001s
2017-10-10 08:57:49,134 Training Iter 2000 Loss 0.028976 in 387.905s
2017-10-10 09:04:16,715 Training Iter 3000 Loss 0.028892 in 387.581s
2017-10-10 09:10:44,187 Training Iter 4000 Loss 0.028872 in 387.472s
2017-10-10 09:14:14,103 Epoch: 81 Train loss 0.0289 | Valid loss 0.0342
2017-10-10 09:20:45,104 Training Iter 1000 Loss 0.028953 in 391.000s
2017-10-10 09:27:12,696 Training Iter 2000 Loss 0.028913 in 387.593s
2017-10-10 09:33:40,646 Training Iter 3000 Loss 0.028872 in 387.949s
2017-10-10 09:40:08,242 Training Iter 4000 Loss 0.028857 in 387.597s
2017-10-10 09:43:38,456 Epoch: 82 Train loss 0.0289 | Valid loss 0.0338
2017-10-10 09:50:09,681 Training Iter 1000 Loss 0.029043 in 391.224s
2017-10-10 09:56:37,200 Training Iter 2000 Loss 0.028936 in 387.519s
2017-10-10 10:03:05,135 Training Iter 3000 Loss 0.028926 in 387.935s
2017-10-10 10:09:32,670 Training Iter 4000 Loss 0.028855 in 387.534s
2017-10-10 10:13:04,707 Epoch: 83 Train loss 0.0289 | Valid loss 0.0335
2017-10-10 10:19:36,005 Training Iter 1000 Loss 0.028807 in 391.298s
2017-10-10 10:26:03,604 Training Iter 2000 Loss 0.028749 in 387.599s
2017-10-10 10:32:31,619 Training Iter 3000 Loss 0.028796 in 388.015s
2017-10-10 10:38:59,252 Training Iter 4000 Loss 0.028856 in 387.633s
2017-10-10 10:42:31,496 Epoch: 84 Train loss 0.0289 | Valid loss 0.0360
2017-10-10 10:49:02,744 Training Iter 1000 Loss 0.028822 in 391.247s
2017-10-10 10:55:30,370 Training Iter 2000 Loss 0.028941 in 387.626s
2017-10-10 11:01:58,030 Training Iter 3000 Loss 0.028902 in 387.661s
2017-10-10 11:08:28,284 Training Iter 4000 Loss 0.028860 in 390.254s
2017-10-10 11:12:10,177 Epoch: 85 Train loss 0.0289 | Valid loss 0.0324
2017-10-10 11:18:45,411 Training Iter 1000 Loss 0.028702 in 395.234s
2017-10-10 11:25:21,407 Training Iter 2000 Loss 0.028815 in 395.996s
2017-10-10 11:32:03,445 Training Iter 3000 Loss 0.028843 in 402.038s
2017-10-10 11:38:47,074 Training Iter 4000 Loss 0.028871 in 403.629s
2017-10-10 11:42:31,294 Epoch: 86 Train loss 0.0289 | Valid loss 0.0331
2017-10-10 11:49:12,875 Training Iter 1000 Loss 0.028733 in 401.580s
2017-10-10 11:55:44,222 Training Iter 2000 Loss 0.028817 in 391.347s
2017-10-10 12:02:14,500 Training Iter 3000 Loss 0.028852 in 390.278s
2017-10-10 12:08:44,739 Training Iter 4000 Loss 0.028850 in 390.239s
2017-10-10 12:12:21,232 Epoch: 87 Train loss 0.0288 | Valid loss 0.0340
2017-10-10 12:18:55,020 Training Iter 1000 Loss 0.028849 in 393.788s
2017-10-10 12:25:25,336 Training Iter 2000 Loss 0.028817 in 390.316s
2017-10-10 12:31:55,534 Training Iter 3000 Loss 0.028828 in 390.197s
2017-10-10 12:38:25,638 Training Iter 4000 Loss 0.028851 in 390.104s
2017-10-10 12:42:02,849 Epoch: 88 Train loss 0.0289 | Valid loss 0.0348
2017-10-10 12:48:36,296 Training Iter 1000 Loss 0.028837 in 393.447s
2017-10-10 12:55:06,995 Training Iter 2000 Loss 0.028853 in 390.699s
2017-10-10 13:01:36,855 Training Iter 3000 Loss 0.028890 in 389.859s
2017-10-10 13:08:06,720 Training Iter 4000 Loss 0.028858 in 389.865s
2017-10-10 13:11:43,323 Epoch: 89 Train loss 0.0288 | Valid loss 0.0325
2017-10-10 13:18:16,481 Training Iter 1000 Loss 0.028991 in 393.158s
2017-10-10 13:24:46,330 Training Iter 2000 Loss 0.028919 in 389.849s
2017-10-10 13:31:16,137 Training Iter 3000 Loss 0.028871 in 389.806s
2017-10-10 13:37:45,953 Training Iter 4000 Loss 0.028866 in 389.816s
2017-10-10 13:41:23,992 Epoch: 90 Train loss 0.0289 | Valid loss 0.0355
2017-10-10 13:48:02,036 Training Iter 1000 Loss 0.028799 in 398.043s
2017-10-10 13:54:31,618 Training Iter 2000 Loss 0.028802 in 389.583s
2017-10-10 14:01:01,316 Training Iter 3000 Loss 0.028816 in 389.697s
2017-10-10 14:07:36,451 Training Iter 4000 Loss 0.028856 in 395.136s
2017-10-10 14:11:58,894 Epoch: 91 Train loss 0.0289 | Valid loss 0.0338
2017-10-10 14:18:52,850 Training Iter 1000 Loss 0.028897 in 413.955s
2017-10-10 14:25:28,570 Training Iter 2000 Loss 0.028825 in 395.720s
2017-10-10 14:32:14,943 Training Iter 3000 Loss 0.028882 in 406.374s
2017-10-10 14:38:57,832 Training Iter 4000 Loss 0.028854 in 402.888s
2017-10-10 14:43:05,216 Epoch: 92 Train loss 0.0289 | Valid loss 0.0338
2017-10-10 14:50:00,531 Training Iter 1000 Loss 0.028898 in 415.314s
2017-10-10 14:56:44,811 Training Iter 2000 Loss 0.028889 in 404.280s
2017-10-10 15:03:39,850 Training Iter 3000 Loss 0.028819 in 415.039s
2017-10-10 15:10:29,624 Training Iter 4000 Loss 0.028860 in 409.774s
2017-10-10 15:14:32,082 Epoch: 93 Train loss 0.0289 | Valid loss 0.0334
2017-10-10 15:21:02,416 Training Iter 1000 Loss 0.028846 in 390.333s
2017-10-10 15:27:40,438 Training Iter 2000 Loss 0.028802 in 398.022s
2017-10-10 15:34:08,802 Training Iter 3000 Loss 0.028839 in 388.365s
2017-10-10 15:40:35,678 Training Iter 4000 Loss 0.028866 in 386.876s
2017-10-10 15:44:13,364 Epoch: 94 Train loss 0.0289 | Valid loss 0.0330
2017-10-10 15:50:44,538 Training Iter 1000 Loss 0.029030 in 391.173s
2017-10-10 15:57:11,833 Training Iter 2000 Loss 0.028935 in 387.295s
2017-10-10 16:03:42,347 Training Iter 3000 Loss 0.028826 in 390.515s
2017-10-10 16:10:09,752 Training Iter 4000 Loss 0.028850 in 387.405s
2017-10-10 16:13:47,077 Epoch: 95 Train loss 0.0289 | Valid loss 0.0323
